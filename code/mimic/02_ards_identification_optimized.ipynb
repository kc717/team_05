{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDS Identification - Optimized Implementation\n",
    "\n",
    "This notebook identifies ARDS patients from our base cohort using the updated ARDS definition:\n",
    "\n",
    "## ARDS Definition (Updated):\n",
    "**ARDS Onset = S/F ratio ‚â§ 315 within 60 minutes of ICU admission + Bilateral infiltrates flag**\n",
    "\n",
    "### Key Changes:\n",
    "- **No patient filtering**: All cohort patients are retained\n",
    "- **ARDS flag**: Binary indicator for ARDS presence\n",
    "- **ARDS onset time**: Timestamp when S/F ‚â§ 315 first occurred within 60min of ICU admission\n",
    "- **Bilateral infiltrates**: Flag from radiology reports (separate indicator)\n",
    "- **Vectorized processing**: Efficient handling of large datasets\n",
    "\n",
    "### Output:\n",
    "- Cohort with ARDS flags and onset times\n",
    "- Bilateral infiltrates detection\n",
    "- Summary statistics by ARDS status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis start time: 2025-07-20 01:37:20.629612\n",
      "Data path: /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "MIMIC_PATH = '/Users/kavenchhikara/Desktop/CLIF/MIMIC-IV-3.1/physionet.org/files'\n",
    "DATA_PATH = '/Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data'\n",
    "\n",
    "print(f\"Analysis start time: {datetime.now()}\")\n",
    "print(f\"Data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Base Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded optimized cohort: 21,590 patients\n",
      "HADM ID: 18,857 \n",
      "STAY ID ID: 21,590 \n",
      "Unique patients: 17,500\n",
      "\n",
      "Cohort overview:\n",
      "Date range: 2110-01-11 10:14:00 to 2211-05-01 06:57:00\n",
      "ICU stays: 21,590\n"
     ]
    }
   ],
   "source": [
    "# Load the optimized base cohort\n",
    "try:\n",
    "    base_cohort = pd.read_parquet(f'{DATA_PATH}/base_cohort_optimized.parquet')\n",
    "    print(f\"Loaded optimized cohort: {len(base_cohort):,} patients\")\n",
    "    print(f\"HADM ID: {base_cohort['hadm_id'].nunique():,} \")\n",
    "    print(f\"STAY ID ID: {base_cohort['stay_id'].nunique():,} \")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to original cohort\n",
    "    base_cohort = pd.read_parquet(f'{DATA_PATH}/base_cohort.parquet')\n",
    "    print(f\"Loaded original cohort: {len(base_cohort):,} patients\")\n",
    "    print(f\"HADM ID: {base_cohort['hadm_id'].nunique():,} \")\n",
    "    print(f\"STAY ID ID: {base_cohort['stay_id'].nunique():,} \")\n",
    "\n",
    "print(f\"Unique patients: {base_cohort['subject_id'].nunique():,}\")\n",
    "\n",
    "# Convert datetime columns\n",
    "datetime_cols = ['admission_dttm', 'discharge_dttm', 'intime', 'outtime']\n",
    "for col in datetime_cols:\n",
    "    if col in base_cohort.columns:\n",
    "        base_cohort[col] = pd.to_datetime(base_cohort[col])\n",
    "\n",
    "# Display cohort info\n",
    "print(f\"\\nCohort overview:\")\n",
    "print(f\"Date range: {base_cohort['admission_dttm'].min()} to {base_cohort['admission_dttm'].max()}\")\n",
    "print(f\"ICU stays: {base_cohort['stay_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract S/F Ratios Within 60 Minutes of ICU Admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target ICU stays: 21,590\n",
      "Loading chartevents for S/F ratio calculation...\n",
      "Extracting SpO2 and FiO2 data within 60 minutes of ICU admission...\n",
      "Chartevents loaded: 432,997,491 rows\n",
      "S/F measurements for cohort: 6,168,848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1701"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define itemids for SpO2 and FiO2\n",
    "SPO2_ITEMIDS = [220277, 224696]  # SpO2 pulse oximetry\n",
    "FIO2_ITEMIDS = [220210, 223835]  # FiO2 (%) and FiO2 (fraction)\n",
    "SF_ITEMIDS = SPO2_ITEMIDS + FIO2_ITEMIDS\n",
    "\n",
    "# Get target ICU stays\n",
    "target_stay_ids = set(base_cohort['stay_id'])\n",
    "print(f\"Target ICU stays: {len(target_stay_ids):,}\")\n",
    "\n",
    "print(\"Loading chartevents for S/F ratio calculation...\")\n",
    "print(\"Extracting SpO2 and FiO2 data within 60 minutes of ICU admission...\")\n",
    "\n",
    "# Load chartevents with optimized column selection\n",
    "chartevents = pd.read_csv(\n",
    "    f'{MIMIC_PATH}/mimiciv/3.1/icu/chartevents.csv.gz',\n",
    "    usecols=['stay_id', 'itemid', 'charttime', 'valuenum'],\n",
    "    dtype={'stay_id': 'int32', 'itemid': 'int32', 'valuenum': 'float32'}\n",
    ")\n",
    "print(f\"Chartevents loaded: {len(chartevents):,} rows\")\n",
    "\n",
    "# Filter to our cohort and S/F parameters (vectorized)\n",
    "sf_data = chartevents[\n",
    "    (chartevents['stay_id'].isin(target_stay_ids)) &\n",
    "    (chartevents['itemid'].isin(SF_ITEMIDS)) &\n",
    "    (chartevents['valuenum'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"S/F measurements for cohort: {len(sf_data):,}\")\n",
    "\n",
    "# Clear original chartevents\n",
    "# del chartevents\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/F measurements in first 60 minutes: 84,846\n",
      "Parameter distribution:\n",
      "param_type\n",
      "fio2    45691\n",
      "spo2    39155\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add parameter type and merge with ICU times (vectorized)\n",
    "sf_data['param_type'] = 'unknown'\n",
    "sf_data.loc[sf_data['itemid'].isin(SPO2_ITEMIDS), 'param_type'] = 'spo2'\n",
    "sf_data.loc[sf_data['itemid'].isin(FIO2_ITEMIDS), 'param_type'] = 'fio2'\n",
    "\n",
    "# Convert charttime and merge with ICU admission times\n",
    "sf_data['charttime'] = pd.to_datetime(sf_data['charttime'])\n",
    "sf_data = sf_data.merge(\n",
    "    base_cohort[['stay_id', 'hadm_id', 'intime']], \n",
    "    on='stay_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate minutes from ICU admission (vectorized)\n",
    "sf_data['minutes_from_icu'] = (\n",
    "    sf_data['charttime'] - sf_data['intime']\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "# Filter to first 60 minutes of ICU admission (vectorized)\n",
    "sf_data_60min = sf_data[\n",
    "    (sf_data['minutes_from_icu'] >= 0) &\n",
    "    (sf_data['minutes_from_icu'] <= 60)\n",
    "].copy()\n",
    "\n",
    "print(f\"S/F measurements in first 60 minutes: {len(sf_data_60min):,}\")\n",
    "print(f\"Parameter distribution:\")\n",
    "print(sf_data_60min['param_type'].value_counts())\n",
    "\n",
    "# Clear full sf_data\n",
    "# del sf_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpO2 measurements (60min): 39,155\n",
      "FiO2 measurements (60min): 45,691\n",
      "Calculating S/F ratios using pivot method...\n",
      "Successful S/F calculations (pivot method): 28,917\n",
      "S/F ratio distribution (60min window):\n",
      "count    28917.000000\n",
      "mean       474.522003\n",
      "std        225.091293\n",
      "min          0.000000\n",
      "25%        357.142853\n",
      "50%        490.000000\n",
      "75%        613.333313\n",
      "max       5000.000000\n",
      "Name: sf_ratio, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate SpO2 and FiO2 data\n",
    "spo2_data = sf_data_60min[sf_data_60min['param_type'] == 'spo2'].copy()\n",
    "fio2_data = sf_data_60min[sf_data_60min['param_type'] == 'fio2'].copy()\n",
    "\n",
    "print(f\"SpO2 measurements (60min): {len(spo2_data):,}\")\n",
    "print(f\"FiO2 measurements (60min): {len(fio2_data):,}\")\n",
    "\n",
    "if len(spo2_data) > 0 and len(fio2_data) > 0:\n",
    "    # Convert FiO2 percentages to fractions (vectorized)\n",
    "    fio2_mask = (sf_data_60min['param_type'] == 'fio2') & (sf_data_60min['valuenum'] > 1)\n",
    "    sf_data_60min.loc[fio2_mask, 'valuenum'] = sf_data_60min.loc[fio2_mask, 'valuenum'] / 100\n",
    "\n",
    "    print(\"Calculating S/F ratios using pivot method...\")\n",
    "\n",
    "    # Pivot to get SpO2 and FiO2 as columns (vectorized)\n",
    "    sf_pivot = sf_data_60min.pivot_table(\n",
    "        index=['stay_id', 'hadm_id', 'charttime'],\n",
    "        columns='param_type',\n",
    "        values='valuenum',\n",
    "        aggfunc='first'  # Take first value if multiple at same time\n",
    "    ).reset_index()\n",
    "\n",
    "    # Clean up column names\n",
    "    sf_pivot.columns.name = None\n",
    "\n",
    "    # Ensure both columns exist\n",
    "    if 'spo2' not in sf_pivot.columns:\n",
    "        sf_pivot['spo2'] = np.nan\n",
    "    if 'fio2' not in sf_pivot.columns:\n",
    "        sf_pivot['fio2'] = np.nan\n",
    "\n",
    "    # Calculate S/F ratios where both measurements exist (vectorized)\n",
    "    sf_ratios = sf_pivot.dropna(subset=['spo2', 'fio2']).copy()\n",
    "    sf_ratios = sf_ratios[sf_ratios['fio2'] > 0]  # Avoid division by zero\n",
    "    sf_ratios['sf_ratio'] = sf_ratios['spo2'] / sf_ratios['fio2']\n",
    "\n",
    "    print(f\"Successful S/F calculations (pivot method): {len(sf_ratios):,}\")\n",
    "    print(f\"S/F ratio distribution (60min window):\")\n",
    "    print(sf_ratios['sf_ratio'].describe())\n",
    "\n",
    "    sf_ratios = sf_ratios.merge(\n",
    "        base_cohort[['stay_id','intime']], \n",
    "        on='stay_id', \n",
    "        how='left')\n",
    "    sf_ratios['minutes_from_icu'] = (sf_ratios['charttime'] - sf_ratios['intime']).dt.total_seconds() / 60\n",
    "\n",
    "    # Clear intermediate data\n",
    "    # del spo2_data, fio2_data, sf_pivot, sf_data_60min\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"Insufficient SpO2/FiO2 data in 60-minute window!\")\n",
    "    sf_ratios = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>fio2</th>\n",
       "      <th>spo2</th>\n",
       "      <th>sf_ratio</th>\n",
       "      <th>intime</th>\n",
       "      <th>minutes_from_icu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000153</td>\n",
       "      <td>23998182</td>\n",
       "      <td>2174-09-29 13:00:00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>100.0</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>2174-09-29 12:09:00</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000646</td>\n",
       "      <td>22795209</td>\n",
       "      <td>2194-04-29 01:41:00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>97.0</td>\n",
       "      <td>346.428558</td>\n",
       "      <td>2194-04-29 01:39:22</td>\n",
       "      <td>1.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30000646</td>\n",
       "      <td>22795209</td>\n",
       "      <td>2194-04-29 02:00:00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>98.0</td>\n",
       "      <td>296.969696</td>\n",
       "      <td>2194-04-29 01:39:22</td>\n",
       "      <td>20.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30001555</td>\n",
       "      <td>25778760</td>\n",
       "      <td>2177-09-27 12:00:00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>96.0</td>\n",
       "      <td>685.714294</td>\n",
       "      <td>2177-09-27 11:23:13</td>\n",
       "      <td>36.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30001947</td>\n",
       "      <td>23836605</td>\n",
       "      <td>2162-12-26 15:17:00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>97.0</td>\n",
       "      <td>692.857117</td>\n",
       "      <td>2162-12-26 15:04:30</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stay_id   hadm_id           charttime  fio2   spo2    sf_ratio  \\\n",
       "0  30000153  23998182 2174-09-29 13:00:00  0.16  100.0  625.000000   \n",
       "1  30000646  22795209 2194-04-29 01:41:00  0.28   97.0  346.428558   \n",
       "2  30000646  22795209 2194-04-29 02:00:00  0.33   98.0  296.969696   \n",
       "3  30001555  25778760 2177-09-27 12:00:00  0.14   96.0  685.714294   \n",
       "4  30001947  23836605 2162-12-26 15:17:00  0.14   97.0  692.857117   \n",
       "\n",
       "               intime  minutes_from_icu  \n",
       "0 2174-09-29 12:09:00         51.000000  \n",
       "1 2194-04-29 01:39:22          1.633333  \n",
       "2 2194-04-29 01:39:22         20.633333  \n",
       "3 2177-09-27 11:23:13         36.783333  \n",
       "4 2162-12-26 15:04:30         12.500000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_ratios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Identify ARDS Onset (S/F ‚â§ 315 within 60 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/F measurements ‚â§ 315: 5,515\n",
      "Admissions with ARDS onset (S/F ‚â§ 315): 4,402\n",
      "\n",
      "ARDS onset timing (minutes from ICU admission):\n",
      "count    4402.000000\n",
      "mean       24.866852\n",
      "std        17.368045\n",
      "min         0.000000\n",
      "25%         9.800000\n",
      "50%        22.000000\n",
      "75%        39.000000\n",
      "max        60.000000\n",
      "Name: ards_onset_minutes_from_icu, dtype: float64\n",
      "\n",
      "ARDS onset S/F ratios:\n",
      "count    4402.000000\n",
      "mean      122.652115\n",
      "std       103.251930\n",
      "min         0.000000\n",
      "25%        30.000000\n",
      "50%        91.485508\n",
      "75%       222.670460\n",
      "max       314.814789\n",
      "Name: ards_onset_sf_ratio, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if len(sf_ratios) > 0:\n",
    "    # Identify ARDS onset: first S/F ratio ‚â§ 315 within 60 minutes (vectorized)\n",
    "    ards_sf_criteria = sf_ratios[sf_ratios['sf_ratio'] <= 315].copy()\n",
    "    \n",
    "    print(f\"S/F measurements ‚â§ 315: {len(ards_sf_criteria):,}\")\n",
    "    \n",
    "    if len(ards_sf_criteria) > 0:\n",
    "        # Get earliest ARDS onset time for each admission (vectorized)\n",
    "        ards_onset_times = ards_sf_criteria.groupby('hadm_id').agg({\n",
    "            'charttime': 'min',  # Earliest time with S/F ‚â§ 315\n",
    "            'sf_ratio': 'min',   # Lowest S/F ratio\n",
    "            'minutes_from_icu': 'min'  # Minutes from ICU admission\n",
    "        }).reset_index()\n",
    "        \n",
    "        ards_onset_times.rename(columns={\n",
    "            'charttime': 'ards_onset_time',\n",
    "            'sf_ratio': 'ards_onset_sf_ratio',\n",
    "            'minutes_from_icu': 'ards_onset_minutes_from_icu'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        print(f\"Admissions with ARDS onset (S/F ‚â§ 315): {len(ards_onset_times):,}\")\n",
    "        print(f\"\\nARDS onset timing (minutes from ICU admission):\")\n",
    "        print(ards_onset_times['ards_onset_minutes_from_icu'].describe())\n",
    "        \n",
    "        print(f\"\\nARDS onset S/F ratios:\")\n",
    "        print(ards_onset_times['ards_onset_sf_ratio'].describe())\n",
    "        \n",
    "    else:\n",
    "        print(\"No ARDS onset events found (S/F ‚â§ 315)!\")\n",
    "        ards_onset_times = pd.DataFrame()\n",
    "        \n",
    "else:\n",
    "    print(\"No S/F ratios calculated - cannot identify ARDS onset!\")\n",
    "    ards_onset_times = pd.DataFrame()\n",
    "\n",
    "# Clear sf_ratios data\n",
    "if 'sf_ratios' in locals():\n",
    "    del sf_ratios, ards_sf_criteria\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Detect Bilateral Infiltrates from Radiology Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading radiology reports for bilateral infiltrates detection...\n",
      "Radiology reports for cohort: 215,193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load radiology reports for cohort\n",
    "print(\"Loading radiology reports for bilateral infiltrates detection...\")\n",
    "radiology = pd.read_csv(f'{MIMIC_PATH}/mimic-iv-note/2.2/note/radiology.csv.gz')\n",
    "\n",
    "# Filter to our cohort admissions\n",
    "cohort_hadm_ids = set(base_cohort['hadm_id'])\n",
    "cohort_radiology = radiology[\n",
    "    radiology['hadm_id'].isin(cohort_hadm_ids)\n",
    "].copy()\n",
    "\n",
    "print(f\"Radiology reports for cohort: {len(cohort_radiology):,}\")\n",
    "\n",
    "# Convert charttime\n",
    "cohort_radiology['charttime'] = pd.to_datetime(cohort_radiology['charttime'])\n",
    "\n",
    "# Clear original radiology data\n",
    "# del radiology\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting bilateral infiltrates in radiology reports...\n",
      "Reports with bilateral infiltrates: 24,035 (11.2%)\n"
     ]
    }
   ],
   "source": [
    "# Vectorized bilateral infiltrates detection\n",
    "def detect_bilateral_infiltrates_vectorized(text_series):\n",
    "    \"\"\"\n",
    "    Vectorized function to detect bilateral infiltrates in radiology reports\n",
    "    \"\"\"\n",
    "    # Convert to lowercase (vectorized)\n",
    "    text_lower = text_series.str.lower().fillna('')\n",
    "    \n",
    "    # Define bilateral patterns\n",
    "    bilateral_patterns = [\n",
    "        r'bilateral.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'(?:infiltrate|opacity|opacities|consolidation).*bilateral',\n",
    "        r'both lung.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'(?:infiltrate|opacity|opacities|consolidation).*both lung',\n",
    "        r'diffuse.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'multifocal.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'bibasilar.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'bilateral.*ground.?glass',\n",
    "        r'bilateral.*airspace disease',\n",
    "        r'ards',  # Direct ARDS mention\n",
    "        r'acute respiratory distress syndrome'\n",
    "    ]\n",
    "    \n",
    "    # Check for any bilateral pattern (vectorized)\n",
    "    bilateral_matches = pd.Series(False, index=text_series.index)\n",
    "    \n",
    "    for pattern in bilateral_patterns:\n",
    "        bilateral_matches |= text_lower.str.contains(pattern, regex=True, na=False)\n",
    "    \n",
    "    # Check for separate left and right infiltrates (vectorized)\n",
    "    left_infiltrate = text_lower.str.contains(\n",
    "        r'left.*(?:infiltrate|opacity|consolidation)', regex=True, na=False\n",
    "    )\n",
    "    right_infiltrate = text_lower.str.contains(\n",
    "        r'right.*(?:infiltrate|opacity|consolidation)', regex=True, na=False\n",
    "    )\n",
    "    \n",
    "    # Combine results\n",
    "    return bilateral_matches | (left_infiltrate & right_infiltrate)\n",
    "\n",
    "# Apply bilateral infiltrates detection (vectorized)\n",
    "print(\"Detecting bilateral infiltrates in radiology reports...\")\n",
    "cohort_radiology['bilateral_infiltrates'] = detect_bilateral_infiltrates_vectorized(\n",
    "    cohort_radiology['text']\n",
    ")\n",
    "\n",
    "bilateral_reports = cohort_radiology['bilateral_infiltrates'].sum()\n",
    "total_reports = len(cohort_radiology)\n",
    "print(f\"Reports with bilateral infiltrates: {bilateral_reports:,} ({bilateral_reports/total_reports*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admissions with bilateral infiltrates: 8,425\n",
      "Total admissions with radiology: 18,857\n",
      "Bilateral infiltrates rate: 44.7%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get bilateral infiltrates flag for each admission (vectorized)\n",
    "bilateral_by_admission = cohort_radiology.groupby('hadm_id').agg({\n",
    "    'bilateral_infiltrates': 'any',  # True if any report shows bilateral infiltrates\n",
    "    'charttime': 'min'  # First radiology report time\n",
    "}).reset_index()\n",
    "\n",
    "bilateral_by_admission.rename(columns={\n",
    "    'bilateral_infiltrates': 'has_bilateral_infiltrates',\n",
    "    'charttime': 'first_radiology_time'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"Admissions with bilateral infiltrates: {bilateral_by_admission['has_bilateral_infiltrates'].sum():,}\")\n",
    "print(f\"Total admissions with radiology: {len(bilateral_by_admission):,}\")\n",
    "print(f\"Bilateral infiltrates rate: {bilateral_by_admission['has_bilateral_infiltrates'].mean()*100:.1f}%\")\n",
    "\n",
    "# Clear radiology data\n",
    "# del cohort_radiology\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Final Dataset with ARDS Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cohort: 21,590 patients\n",
      "Merged ARDS onset data for 4,402 admissions\n",
      "Merged bilateral infiltrates data for 18,857 admissions\n",
      "Final dataset: 21,590 patients\n"
     ]
    }
   ],
   "source": [
    "# Start with base cohort\n",
    "final_cohort = base_cohort.copy()\n",
    "print(f\"Starting cohort: {len(final_cohort):,} patients\")\n",
    "\n",
    "# Merge ARDS onset information (vectorized left join)\n",
    "if len(ards_onset_times) > 0:\n",
    "    final_cohort = final_cohort.merge(\n",
    "        ards_onset_times, \n",
    "        on='hadm_id', \n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"Merged ARDS onset data for {len(ards_onset_times):,} admissions\")\n",
    "else:\n",
    "    # Add empty ARDS columns if no onset data\n",
    "    final_cohort['ards_onset_time'] = pd.NaT\n",
    "    final_cohort['ards_onset_sf_ratio'] = np.nan\n",
    "    final_cohort['ards_onset_minutes_from_icu'] = np.nan\n",
    "    print(\"No ARDS onset data - added empty columns\")\n",
    "\n",
    "# Merge bilateral infiltrates information (vectorized left join)\n",
    "if len(bilateral_by_admission) > 0:\n",
    "    final_cohort = final_cohort.merge(\n",
    "        bilateral_by_admission, \n",
    "        on='hadm_id', \n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"Merged bilateral infiltrates data for {len(bilateral_by_admission):,} admissions\")\n",
    "else:\n",
    "    # Add empty bilateral infiltrates columns\n",
    "    final_cohort['has_bilateral_infiltrates'] = False\n",
    "    final_cohort['first_radiology_time'] = pd.NaT\n",
    "    print(\"No bilateral infiltrates data - added empty columns\")\n",
    "\n",
    "# Fill missing values (vectorized)\n",
    "final_cohort['has_bilateral_infiltrates'] = final_cohort['has_bilateral_infiltrates'].fillna(False)\n",
    "\n",
    "print(f\"Final dataset: {len(final_cohort):,} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ARDS flags...\n",
      "ARDS flags created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create ARDS flags (vectorized)\n",
    "print(\"Creating ARDS flags...\")\n",
    "\n",
    "# ARDS onset flag: S/F ‚â§ 315 within 60 minutes\n",
    "final_cohort['has_ards_onset'] = final_cohort['ards_onset_time'].notna()\n",
    "\n",
    "# Combined ARDS flag: onset + bilateral infiltrates\n",
    "final_cohort['has_ards'] = (\n",
    "    final_cohort['has_ards_onset'] & \n",
    "    final_cohort['has_bilateral_infiltrates']\n",
    ")\n",
    "\n",
    "# Calculate time from ICU admission to ARDS onset (vectorized)\n",
    "final_cohort['hours_icu_to_ards_onset'] = (\n",
    "    final_cohort['ards_onset_time'] - final_cohort['intime']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# ARDS severity based on S/F ratio (vectorized)\n",
    "final_cohort['ards_severity'] = 'none'\n",
    "mask_mild = (final_cohort['has_ards_onset']) & (final_cohort['ards_onset_sf_ratio'] > 235)\n",
    "mask_moderate = (final_cohort['has_ards_onset']) & (final_cohort['ards_onset_sf_ratio'] <= 235) & (final_cohort['ards_onset_sf_ratio'] > 150)\n",
    "mask_severe = (final_cohort['has_ards_onset']) & (final_cohort['ards_onset_sf_ratio'] <= 150)\n",
    "\n",
    "final_cohort.loc[mask_mild, 'ards_severity'] = 'mild'\n",
    "final_cohort.loc[mask_moderate, 'ards_severity'] = 'moderate'\n",
    "final_cohort.loc[mask_severe, 'ards_severity'] = 'severe'\n",
    "\n",
    "print(\"ARDS flags created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ARDS IDENTIFICATION SUMMARY ===\n",
      "\n",
      "üìä Total Cohort: 21,590 patients\n",
      "\n",
      "üîç ARDS Onset (S/F ‚â§ 315 within 60min):\n",
      "  Patients: 5,575 (25.8%)\n",
      "  Onset timing (min from ICU): 24.1 ¬± 17.2\n",
      "  Median onset time: 21.0 minutes\n",
      "  S/F ratio at onset: 129.2 ¬± 105.2\n",
      "\n",
      "ü´Å Bilateral Infiltrates:\n",
      "  Patients: 10,361 (48.0%)\n",
      "\n",
      "üö® ARDS (Onset + Bilateral Infiltrates):\n",
      "  Patients: 3,331 (15.4%)\n",
      "\n",
      "üìà ARDS Severity (among ARDS patients):\n",
      "  Severe: 1,817 (54.5%)\n",
      "  Mild: 969 (29.1%)\n",
      "  Moderate: 545 (16.4%)\n",
      "\n",
      "üè• Clinical Outcomes:\n",
      "  Mortality - ARDS: 26.5%\n",
      "  Mortality - No ARDS: 15.1%\n",
      "  ICU LOS (median) - ARDS: 4.3 days\n",
      "  ICU LOS (median) - No ARDS: 2.4 days\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ARDS IDENTIFICATION SUMMARY ===\")\n",
    "\n",
    "# Overall cohort\n",
    "total_patients = len(final_cohort)\n",
    "print(f\"\\nüìä Total Cohort: {total_patients:,} patients\")\n",
    "\n",
    "# ARDS onset (S/F criteria)\n",
    "ards_onset_count = final_cohort['has_ards_onset'].sum()\n",
    "ards_onset_rate = ards_onset_count / total_patients * 100\n",
    "print(f\"\\nüîç ARDS Onset (S/F ‚â§ 315 within 60min):\")\n",
    "print(f\"  Patients: {ards_onset_count:,} ({ards_onset_rate:.1f}%)\")\n",
    "\n",
    "if ards_onset_count > 0:\n",
    "    onset_times = final_cohort[final_cohort['has_ards_onset']]['ards_onset_minutes_from_icu']\n",
    "    print(f\"  Onset timing (min from ICU): {onset_times.mean():.1f} ¬± {onset_times.std():.1f}\")\n",
    "    print(f\"  Median onset time: {onset_times.median():.1f} minutes\")\n",
    "    \n",
    "    onset_sf = final_cohort[final_cohort['has_ards_onset']]['ards_onset_sf_ratio']\n",
    "    print(f\"  S/F ratio at onset: {onset_sf.mean():.1f} ¬± {onset_sf.std():.1f}\")\n",
    "\n",
    "# Bilateral infiltrates\n",
    "bilateral_count = final_cohort['has_bilateral_infiltrates'].sum()\n",
    "bilateral_rate = bilateral_count / total_patients * 100\n",
    "print(f\"\\nü´Å Bilateral Infiltrates:\")\n",
    "print(f\"  Patients: {bilateral_count:,} ({bilateral_rate:.1f}%)\")\n",
    "\n",
    "# Combined ARDS\n",
    "ards_count = final_cohort['has_ards'].sum()\n",
    "ards_rate = ards_count / total_patients * 100\n",
    "print(f\"\\nüö® ARDS (Onset + Bilateral Infiltrates):\")\n",
    "print(f\"  Patients: {ards_count:,} ({ards_rate:.1f}%)\")\n",
    "\n",
    "# ARDS severity distribution\n",
    "if ards_count > 0:\n",
    "    print(f\"\\nüìà ARDS Severity (among ARDS patients):\")\n",
    "    ards_patients = final_cohort[final_cohort['has_ards']]\n",
    "    severity_counts = ards_patients['ards_severity'].value_counts()\n",
    "    for severity, count in severity_counts.items():\n",
    "        if severity != 'none':\n",
    "            pct = count / ards_count * 100\n",
    "            print(f\"  {severity.capitalize()}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Clinical outcomes by ARDS status\n",
    "print(f\"\\nüè• Clinical Outcomes:\")\n",
    "if 'mortality' in final_cohort.columns:\n",
    "    mortality_ards = final_cohort[final_cohort['has_ards']]['mortality'].mean() * 100\n",
    "    mortality_no_ards = final_cohort[~final_cohort['has_ards']]['mortality'].mean() * 100\n",
    "    print(f\"  Mortality - ARDS: {mortality_ards:.1f}%\")\n",
    "    print(f\"  Mortality - No ARDS: {mortality_no_ards:.1f}%\")\n",
    "\n",
    "if 'icu_los_days' in final_cohort.columns:\n",
    "    los_ards = final_cohort[final_cohort['has_ards']]['icu_los_days'].median()\n",
    "    los_no_ards = final_cohort[~final_cohort['has_ards']]['icu_los_days'].median()\n",
    "    print(f\"  ICU LOS (median) - ARDS: {los_ards:.1f} days\")\n",
    "    print(f\"  ICU LOS (median) - No ARDS: {los_no_ards:.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 25 columns...\n",
      "\n",
      "üíæ ARDS COHORT SAVED\n",
      "File: /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data/ards_cohort_with_flags.parquet\n",
      "Size: 1.7 MB\n",
      "Rows: 21,590\n",
      "Columns: 25\n",
      "Summary saved: /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data/ards_identification_summary.json\n",
      "\n",
      "‚è∞ Analysis completed at: 2025-07-20 01:55:42.538055\n"
     ]
    }
   ],
   "source": [
    "# Select columns for final dataset\n",
    "save_columns = [\n",
    "    # Patient identifiers\n",
    "    'subject_id', 'hadm_id', 'stay_id',\n",
    "    \n",
    "    # Times\n",
    "    'admission_dttm', 'discharge_dttm', 'intime', 'outtime',\n",
    "    \n",
    "    # Demographics\n",
    "    'age_at_admission', 'gender',\n",
    "    \n",
    "    # ARDS variables\n",
    "    'has_ards_onset', 'has_bilateral_infiltrates', 'has_ards',\n",
    "    'ards_onset_time', 'ards_onset_sf_ratio', 'ards_onset_minutes_from_icu',\n",
    "    'hours_icu_to_ards_onset', 'ards_severity',\n",
    "    'first_radiology_time',\n",
    "    \n",
    "    # Clinical variables\n",
    "    'admission_type', 'admission_location', 'discharge_location',\n",
    "    'insurance', 'marital_status'\n",
    "]\n",
    "\n",
    "# Add mortality and LOS if available\n",
    "if 'mortality' in final_cohort.columns:\n",
    "    save_columns.append('mortality')\n",
    "if 'icu_los_days' in final_cohort.columns:\n",
    "    save_columns.append('icu_los_days')\n",
    "\n",
    "# Filter to available columns\n",
    "available_columns = [col for col in save_columns if col in final_cohort.columns]\n",
    "print(f\"Saving {len(available_columns)} columns...\")\n",
    "\n",
    "# Save final dataset\n",
    "output_file = f'{DATA_PATH}/ards_cohort_with_flags.parquet'\n",
    "final_cohort[available_columns].to_parquet(output_file, index=False)\n",
    "\n",
    "# Calculate file size\n",
    "file_size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
    "\n",
    "print(f\"\\nüíæ ARDS COHORT SAVED\")\n",
    "print(f\"File: {output_file}\")\n",
    "print(f\"Size: {file_size_mb:.1f} MB\")\n",
    "print(f\"Rows: {len(final_cohort):,}\")\n",
    "print(f\"Columns: {len(available_columns)}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'total_patients': total_patients,\n",
    "    'ards_onset_patients': int(ards_onset_count),\n",
    "    'bilateral_infiltrates_patients': int(bilateral_count), \n",
    "    'ards_patients': int(ards_count),\n",
    "    'ards_rate_percent': float(ards_rate),\n",
    "    'ards_definition': 'S/F ratio ‚â§ 315 within 60min of ICU admission + bilateral infiltrates',\n",
    "    'severity_distribution': final_cohort['ards_severity'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "import json\n",
    "summary_file = f'{DATA_PATH}/ards_identification_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Summary saved: {summary_file}\")\n",
    "print(f\"\\n‚è∞ Analysis completed at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **ARDS identification completed with updated definition!**\n",
    "\n",
    "### ARDS Definition Applied:\n",
    "- **ARDS Onset**: S/F ratio ‚â§ 315 within 60 minutes of ICU admission\n",
    "- **Bilateral Infiltrates**: Flag from radiology report analysis\n",
    "- **Final ARDS**: Both criteria must be met\n",
    "\n",
    "### Key Features:\n",
    "- **No patient filtering**: All cohort patients retained\n",
    "- **ARDS flags**: Binary indicators for each component\n",
    "- **Onset timing**: Precise timestamps relative to ICU admission\n",
    "- **Severity classification**: Based on S/F ratio thresholds\n",
    "- **Vectorized processing**: Efficient handling of large datasets\n",
    "\n",
    "### Output Variables:\n",
    "- `has_ards_onset`: S/F ‚â§ 315 within 60min\n",
    "- `has_bilateral_infiltrates`: Radiology flag\n",
    "- `has_ards`: Combined ARDS flag\n",
    "- `ards_onset_time`: Timestamp of first S/F ‚â§ 315\n",
    "- `ards_severity`: mild/moderate/severe based on S/F ratio\n",
    "\n",
    "### Next Steps:\n",
    "1. **Proning event extraction** with timing relative to ARDS onset\n",
    "2. **Neuromuscular blockade analysis** with dosing and timing\n",
    "3. **Outcome analysis** comparing ARDS vs non-ARDS patients\n",
    "4. **Statistical modeling** for intervention timing effects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
