{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis: ARDS Intervention Timing and Outcomes\n",
    "\n",
    "This notebook performs regression analysis on ARDS patients to examine the relationship between intervention timing and clinical outcomes.\n",
    "\n",
    "## Analysis Plan:\n",
    "1. **Data Preparation (15 min)**\n",
    "   - Load final datasets from analysis_dataset directory\n",
    "   - Create key timing variables (early_prone, early_nmb)\n",
    "   - Handle missing data\n",
    "   - Create severity groups using APACHE + PF ratio\n",
    "\n",
    "2. **Primary Analysis (30 min)**\n",
    "   - Logistic regression for mortality outcome\n",
    "   - Linear regression for length of stay\n",
    "   - Train on eICU, validate on MIMIC-IV\n",
    "\n",
    "3. **Visualizations (10 min)**\n",
    "   - Kaplan-Meier curves\n",
    "   - Box plots for LOS\n",
    "   - ROC curves\n",
    "   - Correlation heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lifelines in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from lifelines) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from lifelines) (1.16.0)\n",
      "Requirement already satisfied: pandas>=2.1 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from lifelines) (2.3.1)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from lifelines) (3.10.3)\n",
      "Requirement already satisfied: autograd>=1.5 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from lifelines) (1.8.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from lifelines) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from lifelines) (1.2.0)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
      "Requirement already satisfied: narwhals>=1.17 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (1.47.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.17.0rc1 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from pandas>=2.1->lifelines) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from pandas>=2.1->lifelines) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import scipy.stats as stats\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directories\n",
    "import os\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MIMIC dataset loaded: 7,750,657 records\n",
      "   Columns: 37\n",
      "   Unique patients: 4,252\n",
      "\n",
      "âœ… eICU dataset loaded: 1,206,142 records\n",
      "   Columns: 28\n",
      "   Unique patients: 16,269\n",
      "\n",
      "ðŸ“Š Dataset Comparison:\n",
      "   Common columns: 23\n",
      "   MIMIC-only: 14\n",
      "   eICU-only: 5\n",
      "\n",
      "ðŸ” Common columns: ['age_at_admission', 'atracurium_dose', 'cisatracurium_dose', 'disposition_category', 'ecmo_flag', 'fio2_set', 'height_cm', 'hospital_id', 'hospitalization_id', 'new_tracheostomy']...\n"
     ]
    }
   ],
   "source": [
    "# Define data path\n",
    "DATA_PATH = '/Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/analysis_dataset'\n",
    "\n",
    "# Load MIMIC final dataset\n",
    "try:\n",
    "    mimic_df = pd.read_parquet(f'{DATA_PATH}/mimic_final.parquet')\n",
    "    print(f\"âœ… MIMIC dataset loaded: {len(mimic_df):,} records\")\n",
    "    print(f\"   Columns: {len(mimic_df.columns)}\")\n",
    "    print(f\"   Unique patients: {mimic_df['patient_id'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading MIMIC data: {e}\")\n",
    "    mimic_df = None\n",
    "\n",
    "# Load eICU final dataset\n",
    "try:\n",
    "    eicu_df = pd.read_parquet(f'{DATA_PATH}/eicu_final.parquet')\n",
    "    print(f\"\\nâœ… eICU dataset loaded: {len(eicu_df):,} records\")\n",
    "    print(f\"   Columns: {len(eicu_df.columns)}\")\n",
    "    print(f\"   Unique patients: {eicu_df['patient_id'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading eICU data: {e}\")\n",
    "    eicu_df = None\n",
    "\n",
    "# Display column comparison\n",
    "if mimic_df is not None and eicu_df is not None:\n",
    "    mimic_cols = set(mimic_df.columns)\n",
    "    eicu_cols = set(eicu_df.columns)\n",
    "    common_cols = mimic_cols & eicu_cols\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Dataset Comparison:\")\n",
    "    print(f\"   Common columns: {len(common_cols)}\")\n",
    "    print(f\"   MIMIC-only: {len(mimic_cols - eicu_cols)}\")\n",
    "    print(f\"   eICU-only: {len(eicu_cols - mimic_cols)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ” Common columns: {sorted(list(common_cols))[:10]}...\") # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create Key Timing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Timing Variables for MIMIC-IV ===\n",
      "   PF ratio calculated: 2,376 valid values\n",
      "   SF ratio calculated: 1,033,586 valid values\n",
      "   Early prone positioning: 60 patients (1.4%)\n",
      "   Early NMB: 278 patients (6.5%)\n",
      "\n",
      "=== Creating Timing Variables for eICU-CRD ===\n",
      "   PF ratio calculated: 0 valid values\n",
      "   SF ratio calculated: 155,921 valid values\n",
      "   Early prone positioning: 733 patients (4.5%)\n",
      "   Early NMB: 113 patients (0.7%)\n"
     ]
    }
   ],
   "source": [
    "def create_timing_variables(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create key timing variables for ARDS intervention analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with time series data\n",
    "    - dataset_name: String identifier for the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with timing variables added\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n=== Creating Timing Variables for {dataset_name} ===\")\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 1. Calculate PF ratio (PaO2/FiO2)\n",
    "    if 'pao2' in df.columns and 'fio2_set' in df.columns:\n",
    "        # Convert FiO2 from percentage to fraction if needed\n",
    "        fio2_fraction = df['fio2_set'].copy()\n",
    "        fio2_fraction = np.where(fio2_fraction > 1, fio2_fraction / 100, fio2_fraction)\n",
    "        \n",
    "        df_processed['pf_ratio'] = df['pao2'] / fio2_fraction\n",
    "        # Filter unrealistic values\n",
    "        df_processed.loc[df_processed['pf_ratio'] > 600, 'pf_ratio'] = np.nan\n",
    "        df_processed.loc[df_processed['pf_ratio'] < 50, 'pf_ratio'] = np.nan\n",
    "        \n",
    "        print(f\"   PF ratio calculated: {df_processed['pf_ratio'].notna().sum():,} valid values\")\n",
    "    \n",
    "    # 2. Calculate SF ratio as backup\n",
    "    if 'spo2' in df.columns and 'fio2_set' in df.columns:\n",
    "        fio2_fraction = df['fio2_set'].copy()\n",
    "        fio2_fraction = np.where(fio2_fraction > 1, fio2_fraction / 100, fio2_fraction)\n",
    "        \n",
    "        df_processed['sf_ratio'] = df['spo2'] / fio2_fraction\n",
    "        # Filter unrealistic values\n",
    "        df_processed.loc[df_processed['sf_ratio'] > 500, 'sf_ratio'] = np.nan\n",
    "        df_processed.loc[df_processed['sf_ratio'] < 50, 'sf_ratio'] = np.nan\n",
    "        \n",
    "        print(f\"   SF ratio calculated: {df_processed['sf_ratio'].notna().sum():,} valid values\")\n",
    "    \n",
    "    # 3. Create early intervention variables (within 24-48h of ARDS onset)\n",
    "    if 'time_from_ARDS_onset' in df.columns:\n",
    "        # Early prone positioning (within 24-48h)\n",
    "        if 'prone_flag' in df.columns:\n",
    "            early_prone_mask = (\n",
    "                (df['prone_flag'] == 1) & \n",
    "                (df['time_from_ARDS_onset'] >= 0) & \n",
    "                (df['time_from_ARDS_onset'] <= 48)  # Within 48 hours\n",
    "            )\n",
    "            \n",
    "            # Get patients who had early proning\n",
    "            early_prone_patients = df[early_prone_mask]['patient_id'].unique()\n",
    "            df_processed['early_prone'] = df_processed['patient_id'].isin(early_prone_patients).astype(int)\n",
    "            \n",
    "            print(f\"   Early prone positioning: {len(early_prone_patients):,} patients ({len(early_prone_patients)/df['patient_id'].nunique()*100:.1f}%)\")\n",
    "        \n",
    "        # Early neuromuscular blockade (within 24-48h)\n",
    "        nmb_cols = ['cisatracurium_dose', 'vecuronium_dose', 'rocuronium_dose', 'atracurium_dose', 'pancuronium_dose']\n",
    "        existing_nmb_cols = [col for col in nmb_cols if col in df.columns]\n",
    "        \n",
    "        if existing_nmb_cols:\n",
    "            # Check for any NMB use within 48h\n",
    "            nmb_mask = (\n",
    "                (df[existing_nmb_cols] > 0).any(axis=1) & \n",
    "                (df['time_from_ARDS_onset'] >= 0) & \n",
    "                (df['time_from_ARDS_onset'] <= 48)\n",
    "            )\n",
    "            \n",
    "            early_nmb_patients = df[nmb_mask]['patient_id'].unique()\n",
    "            df_processed['early_nmb'] = df_processed['patient_id'].isin(early_nmb_patients).astype(int)\n",
    "            \n",
    "            print(f\"   Early NMB: {len(early_nmb_patients):,} patients ({len(early_nmb_patients)/df['patient_id'].nunique()*100:.1f}%)\")\n",
    "        elif 'nmb_used' in df.columns:\n",
    "            # Use existing nmb_used flag\n",
    "            nmb_mask = (\n",
    "                (df['nmb_used'] == 1) & \n",
    "                (df['time_from_ARDS_onset'] >= 0) & \n",
    "                (df['time_from_ARDS_onset'] <= 48)\n",
    "            )\n",
    "            \n",
    "            early_nmb_patients = df[nmb_mask]['patient_id'].unique()\n",
    "            df_processed['early_nmb'] = df_processed['patient_id'].isin(early_nmb_patients).astype(int)\n",
    "            \n",
    "            print(f\"   Early NMB: {len(early_nmb_patients):,} patients ({len(early_nmb_patients)/df['patient_id'].nunique()*100:.1f}%)\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Create timing variables for both datasets\n",
    "if mimic_df is not None:\n",
    "    mimic_processed = create_timing_variables(mimic_df, 'MIMIC-IV')\n",
    "\n",
    "if eicu_df is not None:\n",
    "    eicu_processed = create_timing_variables(eicu_df, 'eICU-CRD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Patient-Level Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Patient-Level Dataset for MIMIC-IV ===\n",
      "   Patient-level dataset created: 4,252 patients\n",
      "   Columns: ['patient_id', 'age_at_admission', 'sex', 'mortality', 'pf_ratio', 'sf_ratio', 'peep_set', 'fio2_set', 'early_prone', 'early_nmb', 'prone_flag', 'nmb_used', 'new_tracheostomy', 'height_cm', 'weight_kg', 'icu_los_days', 'ventilator_free_days_28', 'dataset']\n",
      "\n",
      "=== Creating Patient-Level Dataset for eICU-CRD ===\n",
      "   Patient-level dataset created: 16,269 patients\n",
      "   Columns: ['patient_id', 'age_at_admission', 'sex', 'pf_ratio', 'sf_ratio', 'fio2_set', 'early_prone', 'early_nmb', 'prone_flag', 'nmb_used', 'new_tracheostomy', 'height_cm', 'weight_kg', 'APACHE', 'peep', 'dataset']\n"
     ]
    }
   ],
   "source": [
    "def create_patient_level_dataset(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create patient-level dataset for regression analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with time series data\n",
    "    - dataset_name: String identifier\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with one row per patient\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n=== Creating Patient-Level Dataset for {dataset_name} ===\")\n",
    "    \n",
    "    # Define aggregation rules\n",
    "    agg_rules = {\n",
    "        # Demographics (first value)\n",
    "        'age_at_admission': 'first',\n",
    "        'sex': 'first',\n",
    "        \n",
    "        # Outcomes (static values - should be same across all records)\n",
    "        'mortality': 'first',\n",
    "        \n",
    "        # Respiratory parameters (mean of valid values)\n",
    "        'pf_ratio': lambda x: x.dropna().mean() if len(x.dropna()) > 0 else np.nan,\n",
    "        'sf_ratio': lambda x: x.dropna().mean() if len(x.dropna()) > 0 else np.nan,\n",
    "        'peep_set': lambda x: x.dropna().mean() if len(x.dropna()) > 0 else np.nan,\n",
    "        'fio2_set': lambda x: x.dropna().mean() if len(x.dropna()) > 0 else np.nan,\n",
    "        \n",
    "        # Interventions (max - if ever used)\n",
    "        'early_prone': 'max',\n",
    "        'early_nmb': 'max',\n",
    "        'prone_flag': 'max',\n",
    "        'nmb_used': 'max',\n",
    "        'new_tracheostomy': 'max',\n",
    "        \n",
    "        # Severity scores\n",
    "        'height_cm': 'first',\n",
    "        'weight_kg': 'first'\n",
    "    }\n",
    "    \n",
    "    # Add dataset-specific columns\n",
    "    if 'APACHE' in df.columns:\n",
    "        agg_rules['APACHE'] = 'first'\n",
    "    \n",
    "    if 'icu_los_days' in df.columns:\n",
    "        agg_rules['icu_los_days'] = 'first'\n",
    "    elif 'hospital_los_days' in df.columns:\n",
    "        agg_rules['hospital_los_days'] = 'first'\n",
    "    \n",
    "    if 'ventilator_free_days_28' in df.columns:\n",
    "        agg_rules['ventilator_free_days_28'] = 'first'\n",
    "    \n",
    "    # Handle peep column naming differences\n",
    "    if 'peep' in df.columns and 'peep_set' not in df.columns:\n",
    "        agg_rules['peep'] = lambda x: x.dropna().mean() if len(x.dropna()) > 0 else np.nan\n",
    "        del agg_rules['peep_set']\n",
    "    \n",
    "    # Filter to only use columns that exist\n",
    "    available_cols = {k: v for k, v in agg_rules.items() if k in df.columns}\n",
    "    \n",
    "    # Aggregate to patient level\n",
    "    patient_df = df.groupby('patient_id').agg(available_cols).reset_index()\n",
    "    \n",
    "    # Add dataset identifier\n",
    "    patient_df['dataset'] = dataset_name\n",
    "    \n",
    "    print(f\"   Patient-level dataset created: {len(patient_df):,} patients\")\n",
    "    print(f\"   Columns: {list(patient_df.columns)}\")\n",
    "    \n",
    "    return patient_df\n",
    "\n",
    "# Create patient-level datasets\n",
    "if mimic_processed is not None:\n",
    "    mimic_patients = create_patient_level_dataset(mimic_processed, 'MIMIC-IV')\n",
    "\n",
    "if eicu_processed is not None:\n",
    "    eicu_patients = create_patient_level_dataset(eicu_processed, 'eICU-CRD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create Severity Groups and Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning MIMIC Dataset ===\n",
      "\n",
      "Data cleaning completed:\n",
      "  Patients: 4,252\n",
      "  Missing values: 4071\n",
      "MIMIC severity groups:\n",
      "ards_severity_pf\n",
      "Moderate    93\n",
      "Mild        81\n",
      "No_ARDS     75\n",
      "Severe      19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Cleaning eICU Dataset ===\n",
      "\n",
      "Data cleaning completed:\n",
      "  Patients: 16,269\n",
      "  Missing values: 39849\n",
      "eICU severity groups:\n",
      "ards_severity_pf\n",
      "Severe      0\n",
      "Moderate    0\n",
      "Mild        0\n",
      "No_ARDS     0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_severity_groups_and_clean(df):\n",
    "    \"\"\"\n",
    "    Create severity groups and handle missing data\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Patient-level DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned DataFrame with severity groups\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Create PF ratio severity groups (Berlin criteria)\n",
    "    if 'pf_ratio' in df_clean.columns:\n",
    "        df_clean['ards_severity_pf'] = pd.cut(\n",
    "            df_clean['pf_ratio'],\n",
    "            bins=[0, 100, 200, 300, np.inf],\n",
    "            labels=['Severe', 'Moderate', 'Mild', 'No_ARDS'],\n",
    "            ordered=True\n",
    "        )\n",
    "    \n",
    "    # 2. Create SF ratio severity groups (approximation)\n",
    "    if 'sf_ratio' in df_clean.columns:\n",
    "        df_clean['ards_severity_sf'] = pd.cut(\n",
    "            df_clean['sf_ratio'],\n",
    "            bins=[0, 88, 181, 315, np.inf],\n",
    "            labels=['Severe', 'Moderate', 'Mild', 'No_ARDS'],\n",
    "            ordered=True\n",
    "        )\n",
    "    \n",
    "    # 4. Handle missing data with simple imputation\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    valid_numeric_cols = [col for col in numeric_cols if df_clean[col].notna().any()]\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Impute numeric variables with median (only on valid cols)\n",
    "    imputer_numeric = SimpleImputer(strategy='median')\n",
    "    df_clean[valid_numeric_cols] = imputer_numeric.fit_transform(df_clean[valid_numeric_cols])\n",
    "    \n",
    "    # Impute categorical variables with mode\n",
    "    imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "    df_clean[categorical_cols] = imputer_categorical.fit_transform(df_clean[categorical_cols])\n",
    "    \n",
    "    # 5. Create additional derived variables\n",
    "    # BMI if height and weight available\n",
    "    if 'height_cm' in df_clean.columns and 'weight_kg' in df_clean.columns:\n",
    "        df_clean['bmi'] = df_clean['weight_kg'] / (df_clean['height_cm'] / 100) ** 2\n",
    "        df_clean['obesity'] = (df_clean['bmi'] >= 30).astype(int)\n",
    "    \n",
    "    # Gender encoding\n",
    "    if 'sex' in df_clean.columns:\n",
    "        df_clean['male'] = (df_clean['sex'].str.upper().isin(['M', 'MALE'])).astype(int)\n",
    "    \n",
    "    print(f\"\\nData cleaning completed:\")\n",
    "    print(f\"  Patients: {len(df_clean):,}\")\n",
    "    print(f\"  Missing values: {df_clean.isnull().sum().sum()}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean both datasets\n",
    "if mimic_patients is not None:\n",
    "    print(\"\\n=== Cleaning MIMIC Dataset ===\")\n",
    "    mimic_clean = create_severity_groups_and_clean(mimic_patients)\n",
    "    print(f\"MIMIC severity groups:\")\n",
    "    if 'ards_severity_pf' in mimic_clean.columns:\n",
    "        print(mimic_clean['ards_severity_pf'].value_counts())\n",
    "\n",
    "if eicu_patients is not None:\n",
    "    print(\"\\n=== Cleaning eICU Dataset ===\")\n",
    "    eicu_clean = create_severity_groups_and_clean(eicu_patients)\n",
    "    print(f\"eICU severity groups:\")\n",
    "    if 'ards_severity_pf' in eicu_clean.columns:\n",
    "        print(eicu_clean['ards_severity_pf'].value_counts())\n",
    "    if 'severity_group' in eicu_clean.columns:\n",
    "        print(f\"Combined severity groups:\")\n",
    "        print(eicu_clean['severity_group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Prepare Analysis Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospital_id               int64\n",
       "patient_id              float64\n",
       "hospitalization_id        int64\n",
       "recorded_dttm            object\n",
       "ARDS_onset_time           int64\n",
       "time_from_ARDS_onset    float64\n",
       "APACHE                  float64\n",
       "sex                      object\n",
       "age_at_admission          int64\n",
       "ethinicity               object\n",
       "disposition_category     object\n",
       "respiratory_device       object\n",
       "ecmo_flag                 int64\n",
       "pao2                    float64\n",
       "fio2_set                float64\n",
       "lmp_set                 float64\n",
       "spo2                    float64\n",
       "peep                    float64\n",
       "height_cm               float64\n",
       "weight_kg               float64\n",
       "nmb_used                float64\n",
       "cisatracurium_dose      float64\n",
       "vecuronium_dose         float64\n",
       "rocuronium_dose         float64\n",
       "atracurium_dose         float64\n",
       "pancuronium_dose        float64\n",
       "prone_flag                int64\n",
       "new_tracheostomy          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eicu_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key predictors: ['early_prone', 'early_nmb', 'age_at_admission', 'male', 'pf_ratio', 'sf_ratio', 'APACHE']\n",
      "Outcomes: ['mortality', 'icu_los_days']\n",
      "\n",
      "MIMIC available predictors: ['early_prone', 'early_nmb', 'age_at_admission', 'male', 'pf_ratio', 'sf_ratio']\n",
      "MIMIC sample characteristics:\n",
      "  Early prone: 60.0 (1.4%)\n",
      "  Early NMB: 278.0 (6.5%)\n",
      "  Mortality: 999.0 (23.5%)\n",
      "\n",
      "eICU available predictors: ['early_prone', 'early_nmb', 'age_at_admission', 'male', 'pf_ratio', 'sf_ratio', 'APACHE']\n",
      "eICU sample characteristics:\n",
      "  Early prone: 733.0 (4.5%)\n",
      "  Early NMB: 113.0 (0.7%)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mortality'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'mortality'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Early prone: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meicu_clean[\u001b[33m'\u001b[39m\u001b[33mearly_prone\u001b[39m\u001b[33m'\u001b[39m].sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meicu_clean[\u001b[33m'\u001b[39m\u001b[33mearly_prone\u001b[39m\u001b[33m'\u001b[39m].mean()*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Early NMB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meicu_clean[\u001b[33m'\u001b[39m\u001b[33mearly_nmb\u001b[39m\u001b[33m'\u001b[39m].sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meicu_clean[\u001b[33m'\u001b[39m\u001b[33mearly_nmb\u001b[39m\u001b[33m'\u001b[39m].mean()*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Mortality: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43meicu_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmortality\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meicu_clean[\u001b[33m'\u001b[39m\u001b[33mmortality\u001b[39m\u001b[33m'\u001b[39m].mean()*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'mortality'"
     ]
    }
   ],
   "source": [
    "# Define key variables for analysis\n",
    "key_predictors = [\n",
    "    'early_prone', 'early_nmb', 'age_at_admission', 'male',\n",
    "    'pf_ratio', 'sf_ratio'\n",
    "]\n",
    "\n",
    "# Define outcomes\n",
    "outcomes = ['mortality']\n",
    "\n",
    "# Add LOS if available\n",
    "if mimic_clean is not None and 'icu_los_days' in mimic_clean.columns:\n",
    "    outcomes.append('icu_los_days')\n",
    "if eicu_clean is not None and 'hospital_los_days' in eicu_clean.columns:\n",
    "    outcomes.append('hospital_los_days')\n",
    "\n",
    "print(f\"Key predictors: {key_predictors}\")\n",
    "print(f\"Outcomes: {outcomes}\")\n",
    "\n",
    "# Check data availability\n",
    "if mimic_clean is not None:\n",
    "    available_predictors_mimic = [col for col in key_predictors if col in mimic_clean.columns]\n",
    "    print(f\"\\nMIMIC available predictors: {available_predictors_mimic}\")\n",
    "    print(f\"MIMIC sample characteristics:\")\n",
    "    print(f\"  Early prone: {mimic_clean['early_prone'].sum()} ({mimic_clean['early_prone'].mean()*100:.1f}%)\")\n",
    "    print(f\"  Early NMB: {mimic_clean['early_nmb'].sum()} ({mimic_clean['early_nmb'].mean()*100:.1f}%)\")\n",
    "    print(f\"  Mortality: {mimic_clean['mortality'].sum()} ({mimic_clean['mortality'].mean()*100:.1f}%)\")\n",
    "\n",
    "if eicu_clean is not None:\n",
    "    available_predictors_eicu = [col for col in key_predictors if col in eicu_clean.columns]\n",
    "    print(f\"\\neICU available predictors: {available_predictors_eicu}\")\n",
    "    print(f\"eICU sample characteristics:\")\n",
    "    print(f\"  Early prone: {eicu_clean['early_prone'].sum()} ({eicu_clean['early_prone'].mean()*100:.1f}%)\")\n",
    "    print(f\"  Early NMB: {eicu_clean['early_nmb'].sum()} ({eicu_clean['early_nmb'].mean()*100:.1f}%)\")\n",
    "    print(f\"  Mortality: {eicu_clean['mortality'].sum()} ({eicu_clean['mortality'].mean()*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
